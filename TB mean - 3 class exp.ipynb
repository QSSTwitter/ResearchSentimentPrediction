{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckkiuRBKkaag"
   },
   "source": [
    "# Tb Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZ1cFyF5kaah"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features_and_target = pd.read_csv('data_with_everything.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kXqRYyEkaak",
    "outputId": "4672ca48-7948-4003-eeec-4aded23b4f62"
   },
   "outputs": [],
   "source": [
    "\n",
    "features_and_target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cx0DNEWkaan",
    "outputId": "ebb1f39e-60a3-4823-c379-f4b63ae8f048"
   },
   "outputs": [],
   "source": [
    "final_score_list=[]\n",
    "for val in features_and_target['Tw_TB_mean']:\n",
    "    if val > 0:\n",
    "        final_score_list.append(1)\n",
    "    elif val < 0:\n",
    "        final_score_list.append(-1)\n",
    "    else:\n",
    "        final_score_list.append(0)\n",
    "\n",
    "print(len(final_score_list))\n",
    "\n",
    "features_and_target = features_and_target.assign(class_label_Tb_mean=final_score_list)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dB-eT-fVkaaq",
    "outputId": "321badd7-a2fe-41a7-a273-7b776e994fe5"
   },
   "outputs": [],
   "source": [
    "features_and_target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwY4MEqakaat"
   },
   "outputs": [],
   "source": [
    "\n",
    "features_and_target_to_use = features_and_target[['Scopus_Subjects','title_TB','abstract_TB',\\\n",
    "                                                  'Count_HashTags','Abstract_Length','followers_count','author_count',\\\n",
    "                                                  'class_label_Tb_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaedD1f9kaaw",
    "outputId": "a46e15ea-e3bc-4bb5-9893-cc0cba3f6218"
   },
   "outputs": [],
   "source": [
    "# observing the counts of positive and negative\n",
    "from collections import Counter\n",
    "tw_Tb_mean_class_list =features_and_target_to_use['class_label_Tb_mean'].tolist()\n",
    "a = dict(Counter(tw_Tb_mean_class_list))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpjs0UYzkaa0"
   },
   "outputs": [],
   "source": [
    "features_and_target_to_use.shape\n",
    "features_and_target_to_use_2_labels = features_and_target_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lj7W6hYkaa3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# considering only +1 and -1\n",
    "features_and_target_to_use_2_labels = features_and_target_to_use[features_and_target_to_use.class_label_Tb_mean != 0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTNfG37skaa6"
   },
   "outputs": [],
   "source": [
    "features_and_target_to_use_2_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XD93jVnkaa9"
   },
   "outputs": [],
   "source": [
    "# observing the counts of positive and negative\n",
    "from collections import Counter\n",
    "tw_Tb_mean_class_list =features_and_target_to_use_2_labels['class_label_Tb_mean'].tolist()\n",
    "a = dict(Counter(tw_Tb_mean_class_list))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_target_to_use_2_labels.columns  = ['Scopus Subjects', 'Title Sentiment','Abstract Sentiment', 'Hashtag Count','Abstract Length','Tweet Reach','Author Count','Tweet Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COU3BXelkabB",
    "outputId": "1a0b71ec-30b2-4b67-ea07-94e2de92274a"
   },
   "outputs": [],
   "source": [
    "features_and_target_to_use_2_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WopSMcmkabD",
    "outputId": "30b2bf51-f72c-4fa2-abfc-9674d61ae76f"
   },
   "outputs": [],
   "source": [
    "features_and_target_to_use_2_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5-byXiAkabG",
    "outputId": "a407889d-d49e-478e-d41a-391472cd48ac"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "corrMatrix = features_and_target_to_use_2_labels.corr()\n",
    "sn.heatmap(corrMatrix, annot=True, cmap='coolwarm')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IFPWaJ-kabI",
    "outputId": "6c19854f-71a4-4b65-b873-1a16eaff12dc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Seaborn visualization library\n",
    "import seaborn as sns\n",
    "# Create the default pairplot\n",
    "sns.pairplot(features_and_target_to_use_2_labels, hue='class_label_Tb_mean')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqYDOlqJkabP"
   },
   "source": [
    "Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oG1Der65kabQ",
    "outputId": "a3473690-5309-42a1-cf0d-24a19bfb7d9b"
   },
   "outputs": [],
   "source": [
    "print(features_and_target_to_use_2_labels['Scopus Subjects'].unique())\n",
    "\n",
    "print(len(features_and_target_to_use_2_labels['Scopus Subjects'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdO0lKcNkabT",
    "outputId": "093068e5-7002-4340-fa11-a151cd627815"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier(criterion='entropy', max_depth=14, min_samples_split=600)\n",
    "DT.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predDT = DT.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predDT)\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predDT))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predDT))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predDT))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predDT))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = DT, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'criterion': [\"gini\"], 'max_depth': [1,2,3,14,15], 'min_samples_split': [4,6,8,300,450,600]},\n",
    "               {'criterion': [\"entropy\"], 'max_depth': [1,2,13,14,15], 'min_samples_split': [4,6,250,425,450,475,575,600]}]\n",
    "grid_search = GridSearchCV(estimator = DT,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1, verbose=1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predDT)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters')\n",
    "print(best_parameters)\n",
    "print()\n",
    "\n",
    "print('feature importances')\n",
    "print(features_and_target_to_use_2_labels.columns)\n",
    "print(DT.feature_importances_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImvtgvV9kabW",
    "outputId": "06fd41b3-de5a-4533-8da9-d768b7894b2d"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth= 120, min_samples_split=80, n_estimators=40)\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predRF = RF.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predRF)\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predRF))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predRF))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predRF))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predRF))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = RF, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'criterion': [\"entropy\"], 'max_depth': [10,20,40,60,80,100,120], 'min_samples_split': [20,40,60,80], 'n_estimators':[10,20,30,40], 'bootstrap':[False]}]\n",
    "grid_search = GridSearchCV(estimator = RF,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1, verbose=1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predRF)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters')\n",
    "print(best_parameters)\n",
    "print()\n",
    "\n",
    "print('feature importances')\n",
    "print(features_and_target_to_use_2_labels.columns)\n",
    "print(RF.feature_importances_)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WO7ZfMaekabY",
    "outputId": "d80502ec-cb7b-4ef7-ef69-4040159685a2"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "##Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C = 0.1, multi_class='ovr', penalty='l1', solver= 'liblinear')\n",
    "LR.fit(X_train, y_train)\n",
    "y_predLR = LR.predict(X_test)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predLR)\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predLR))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predLR))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predLR))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predLR))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = 10)\n",
    "accuracies.mean()\n",
    "\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'penalty': [\"l1\"], 'C': [0.1,0.2,0.5,0.8,1,1.2,1.4], 'solver':['liblinear','saga'], 'multi_class':['ovr']},\n",
    "             {'penalty': [\"l2\"], 'C': [0.1,0.2,0.5,0.8,1,1.2,1.4], 'solver':['newton-cg','lbfgs','sag'], 'multi_class':['ovr', 'multinomial']}\n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = LR,\n",
    "                          param_grid = parameters,\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 10,\n",
    "                          n_jobs = -1, verbose=1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predLR)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters')\n",
    "print(best_parameters)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIM2oPSSkaba",
    "outputId": "e917b946-7d11-4f69-8775-3c06716a95fc"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predNB = NB.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predNB)\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predNB))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predNB))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predNB))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predNB))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = NB, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predNB)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QvMz4ViDkabd",
    "outputId": "9c491176-1549-42b7-dfe5-282779ec17aa"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier(n_neighbors= 160, algorithm= 'auto')\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predKNN = KNN.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predKNN)\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predKNN))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predKNN))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predKNN))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predKNN))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = KNN, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'n_neighbors': [5,10,20,25,30,40,50,100,120,140,160,180,200,220,240], 'algorithm': ['ball_tree','kd_tree']}]\n",
    "grid_search = GridSearchCV(estimator = KNN,\n",
    "                          param_grid = parameters,\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 10,\n",
    "                          n_jobs = -1, verbose=1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predKNN)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters')\n",
    "print(best_parameters)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XC6QgAGykabf",
    "outputId": "d716c301-9279-4e1b-a66e-fabede04fe5a"
   },
   "outputs": [],
   "source": [
    "\"\"\"PLOTTING\"\"\"\n",
    "'''\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(0).clf()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "\n",
    "x = [0.0, 1.0]\n",
    "plt.plot(x, x, linestyle='dashed', color='red', linewidth=2, label='random')\n",
    "\n",
    "#RF\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predRF)\n",
    "auc = metrics.roc_auc_score(y_test, y_predRF)\n",
    "plt.plot(fpr,tpr,label=\"Random Forest, auc=\"+str(auc)[:5])\n",
    "\n",
    "#DT\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predDT)\n",
    "auc = metrics.roc_auc_score(y_test, y_predDT)\n",
    "plt.plot(fpr,tpr,label=\"Decision Tree, auc=\"+str(auc)[:5])\n",
    "\n",
    "#KNN\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predKNN)\n",
    "auc = metrics.roc_auc_score(y_test, y_predKNN)\n",
    "plt.plot(fpr,tpr,label=\"KNN, auc=\"+str(auc)[:5])\n",
    "\n",
    "#LR\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predLR)\n",
    "auc = metrics.roc_auc_score(y_test, y_predLR)\n",
    "plt.plot(fpr,tpr,label=\"Logistic Regression, auc=\"+str(auc)[:5])\n",
    "\n",
    "#NB\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predNB)\n",
    "auc = metrics.roc_auc_score(y_test, y_predNB)\n",
    "plt.plot(fpr,tpr,label=\"Naive Bayes, auc=\"+str(auc)[:5])\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SP1zq733kabj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TB mean - 3 class exp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
