{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLw0ntI2b-Rh"
   },
   "source": [
    "# Vader Median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwh14kspb-Rk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features_and_target = pd.read_csv('data_with_everything.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nH7WWILb-Ro",
    "outputId": "bd266572-7672-423d-c3c1-91f776ed6e0b"
   },
   "outputs": [],
   "source": [
    "\n",
    "features_and_target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5eXvls6nb-Ru",
    "outputId": "2536b8c7-94c7-4abd-de73-1d373a28208a"
   },
   "outputs": [],
   "source": [
    "final_score_list=[]\n",
    "for val in features_and_target['Tw_Vader_median']:\n",
    "    if val > 0:\n",
    "        final_score_list.append(1)\n",
    "    elif val < 0:\n",
    "        final_score_list.append(-1)\n",
    "    else:\n",
    "        final_score_list.append(0)\n",
    "\n",
    "print(len(final_score_list))\n",
    "\n",
    "features_and_target = features_and_target.assign(class_label_Vader_median=final_score_list)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGiTmv12b-R0",
    "outputId": "a9782b49-633b-432e-8660-d19c31c299fd"
   },
   "outputs": [],
   "source": [
    "features_and_target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aMSOVM5b-R5"
   },
   "outputs": [],
   "source": [
    "\n",
    "features_and_target_to_use = features_and_target[['Scopus_Subjects','title_Vader','abstract_Vader',\\\n",
    "                                                  'Count_HashTags','Abstract_Length','followers_count','author_count',\\\n",
    "                                                  'class_label_Vader_median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFhA5UNGb-R_",
    "outputId": "a12db963-d60a-4870-fa35-bc16e9ca2817"
   },
   "outputs": [],
   "source": [
    "# observing the counts of positive and negative\n",
    "from collections import Counter\n",
    "tw_Vader_median_class_list =features_and_target_to_use['class_label_Vader_median'].tolist()\n",
    "a = dict(Counter(tw_Vader_median_class_list))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSPrru9Yb-SF"
   },
   "outputs": [],
   "source": [
    "features_and_target_to_use.shape\n",
    "features_and_target_to_use_2_labels = features_and_target_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-BKk-enb-SM"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# considering only +1 and -1\n",
    "features_and_target_to_use_2_labels = features_and_target_to_use[features_and_target_to_use.class_label_Vader_median != 0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DCfA5H5b-ST"
   },
   "outputs": [],
   "source": [
    "features_and_target_to_use_2_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IK0t1MTqb-Sc",
    "outputId": "22be9fcc-7a1a-452f-8dc0-92dc40af0d88"
   },
   "outputs": [],
   "source": [
    "# observing the counts of positive and negative\n",
    "from collections import Counter\n",
    "fb_Vader_mean_class_list =features_and_target_to_use_2_labels['class_label_Vader_median'].tolist()\n",
    "a = dict(Counter(fb_Vader_mean_class_list))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_target_to_use_2_labels.columns  = ['Scopus Subjects', 'Title Sentiment','Abstract Sentiment', 'Hashtag Count','Abstract Length','Tweet Reach','Author Count','Tweet Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9aH75N2bb-Sn",
    "outputId": "8fdb553c-1e80-411c-c1bb-8fa56d328eb3"
   },
   "outputs": [],
   "source": [
    "features_and_target_to_use_2_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTe3pxTkb-St",
    "outputId": "d4ff9b6e-024e-43aa-98f0-d820b152d59f"
   },
   "outputs": [],
   "source": [
    "features_and_target_to_use_2_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPyvugCFb-S0",
    "outputId": "820c3d31-6fb3-4b16-a962-5814433ea939"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "corrMatrix = features_and_target_to_use_2_labels.corr()\n",
    "sn.heatmap(corrMatrix, annot=True, cmap='coolwarm')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GY0Dff60b-S7",
    "outputId": "9c4af2eb-cf1a-435c-d26f-084f51cac361"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Seaborn visualization library\n",
    "import seaborn as sns\n",
    "# Create the default pairplot\n",
    "sns.pairplot(features_and_target_to_use_2_labels, hue='class_label_Vader_median')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RX6y2a15b-TC"
   },
   "source": [
    "Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QM-79Ddub-TF",
    "outputId": "f7271705-24cf-4830-fd7f-10e8c2d18777"
   },
   "outputs": [],
   "source": [
    "print(features_and_target_to_use_2_labels['Scopus Subjects'].unique())\n",
    "\n",
    "print(len(features_and_target_to_use_2_labels['Scopus Subjects'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Y0ZbH-lb-TM",
    "outputId": "48f811ba-0aa3-4ea1-99c1-21ca88f0d0b9"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier(criterion='gini', max_depth=4, min_samples_split=4)\n",
    "DT.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predDT = DT.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predDT)\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predDT))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predDT))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predDT))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predDT))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = DT, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'criterion': [\"gini\"], 'max_depth': [1,2,3,14,15], 'min_samples_split': [4,6,8,300,450,600]},\n",
    "               {'criterion': [\"entropy\"], 'max_depth': [1,2,13,14,15], 'min_samples_split': [4,6,250,425,450,475,575,600]}]\n",
    "grid_search = GridSearchCV(estimator = DT,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1, verbose=1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predDT)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters')\n",
    "print(best_parameters)\n",
    "print()\n",
    "\n",
    "print('feature importances')\n",
    "print(features_and_target_to_use_2_labels.columns)\n",
    "print(DT.feature_importances_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swwhw2LSb-TU",
    "outputId": "8bab7dad-d3fc-4fa2-be22-a0d6f42efe40"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth= 20, min_samples_split=600, n_estimators=60)\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predRF = RF.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predRF)\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predRF))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predRF))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predRF))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predRF))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = RF, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'criterion': [\"entropy\"], 'max_depth': [10,20,40,60,80,100,120], 'min_samples_split': [20,40,60,80], 'n_estimators':[10,20,30,40], 'bootstrap':[False]}]\n",
    "grid_search = GridSearchCV(estimator = RF,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1, verbose=1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predRF)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters')\n",
    "print(best_parameters)\n",
    "print()\n",
    "\n",
    "print('feature importances')\n",
    "print(features_and_target_to_use_2_labels.columns)\n",
    "print(RF.feature_importances_)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4rslZ-7b-Tb",
    "outputId": "04018a6c-9af8-4bdc-9a1c-9cbc109c2d3f"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "##Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C = 0.1, multi_class='ovr', penalty='l1', solver= 'liblinear')\n",
    "LR.fit(X_train, y_train)\n",
    "y_predLR = LR.predict(X_test)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predLR)\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predLR))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predLR))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predLR))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predLR))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = 10)\n",
    "accuracies.mean()\n",
    "\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'penalty': [\"l1\"], 'C': [0.1,0.2,0.5,0.8,1,1.2,1.4], 'solver':['liblinear','saga'], 'multi_class':['ovr']},\n",
    "             {'penalty': [\"l2\"], 'C': [0.1,0.2,0.5,0.8,1,1.2,1.4], 'solver':['newton-cg','lbfgs','sag'], 'multi_class':['ovr', 'multinomial']}\n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = LR,\n",
    "                          param_grid = parameters,\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 10,\n",
    "                          n_jobs = -1, verbose=1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predLR)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters')\n",
    "print(best_parameters)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8N6Mc4ib-Th",
    "outputId": "ae5c9157-4f93-4f05-e872-3dcd7c783629"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predNB = NB.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predNB)\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predNB))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predNB))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predNB))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predNB))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = NB, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predNB)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8Y1P9PLb-Tn",
    "outputId": "ecd2c3a7-69b0-4788-f15b-93b050117def"
   },
   "outputs": [],
   "source": [
    "# taking features\n",
    "X = features_and_target_to_use_2_labels.iloc[:, :-1].values\n",
    "y = features_and_target_to_use_2_labels.iloc[:, 7].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "# observing the counts of positive and negative before SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# applying SMOTE\n",
    "smt = SMOTE()\n",
    "X, y = smt.fit_sample(X, y)\n",
    "# checking the rows\n",
    "# observing the counts of positive and negative after SMOTE\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "\n",
    "# splitiing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "##Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier(n_neighbors= 160, algorithm= 'auto')\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predKNN = KNN.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predKNN)\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "print('Accuracy',sklearn.metrics.accuracy_score(y_test, y_predKNN))\n",
    "#print('Precision',sklearn.metrics.precision_score(y_test, y_predKNN))\n",
    "#print('Recall',sklearn.metrics.recall_score(y_test, y_predKNN))\n",
    "#print('F-1 score',sklearn.metrics.f1_score(y_test, y_predKNN))\n",
    "\n",
    "print()\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = KNN, X = X_train, y = y_train, cv = 10)\n",
    "print('accuracies mean',accuracies.mean())\n",
    "print('accuracies with 2 SD', accuracies.std() * 2)\n",
    "print()\n",
    "\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'n_neighbors': [5,10,20,25,30,40,50,100,120,140,160,180,200,220,240], 'algorithm': ['ball_tree','kd_tree']}]\n",
    "grid_search = GridSearchCV(estimator = KNN,\n",
    "                          param_grid = parameters,\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 10,\n",
    "                          n_jobs = -1, verbose=1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test, y_predKNN)\n",
    "\n",
    "print()\n",
    "print('classification report')\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "print('best accuracy', best_accuracy)\n",
    "print('best parameters')\n",
    "print(best_parameters)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ots8q1tb-Tv"
   },
   "outputs": [],
   "source": [
    "\"\"\"PLOTTING\"\"\"\n",
    "'''\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(0).clf()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "\n",
    "x = [0.0, 1.0]\n",
    "plt.plot(x, x, linestyle='dashed', color='red', linewidth=2, label='random')\n",
    "\n",
    "#RF\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predRF)\n",
    "auc = metrics.roc_auc_score(y_test, y_predRF)\n",
    "plt.plot(fpr,tpr,label=\"Random Forest, auc=\"+str(auc)[:5])\n",
    "\n",
    "#DT\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predDT)\n",
    "auc = metrics.roc_auc_score(y_test, y_predDT)\n",
    "plt.plot(fpr,tpr,label=\"Decision Tree, auc=\"+str(auc)[:5])\n",
    "\n",
    "#KNN\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predKNN)\n",
    "auc = metrics.roc_auc_score(y_test, y_predKNN)\n",
    "plt.plot(fpr,tpr,label=\"KNN, auc=\"+str(auc)[:5])\n",
    "\n",
    "#LR\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predLR)\n",
    "auc = metrics.roc_auc_score(y_test, y_predLR)\n",
    "plt.plot(fpr,tpr,label=\"Logistic Regression, auc=\"+str(auc)[:5])\n",
    "\n",
    "#NB\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_predNB)\n",
    "auc = metrics.roc_auc_score(y_test, y_predNB)\n",
    "plt.plot(fpr,tpr,label=\"Naive Bayes, auc=\"+str(auc)[:5])\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Kl1CMRtb-T1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Vader Median - 3 class exp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
